{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Oeho0s7H-sMM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Identification of Frost in Martian HiRISE Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3P5rqg_T-sMP"
   },
   "outputs": [],
   "source": [
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    format='%(asctime)s | %(levelname)-5s | %(module)-15s | %(message)s')\n",
    "\n",
    "IMAGE_SIZE = (299, 299)  # All images contained in this dataset are 299x299 (originally, to match Inception v3 input size)\n",
    "SEED = 17\n",
    "\n",
    "# Head directory containing all image subframes. Update with the relative path of your data directory\n",
    "data_head_dir = Path('./data')\n",
    "\n",
    "# Find all subframe directories\n",
    "subdirs = [Path(subdir.stem) for subdir in data_head_dir.iterdir() if subdir.is_dir()]\n",
    "src_image_ids = ['_'.join(a_path.name.split('_')[:3]) for a_path in subdirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b2QZ3OH7-sMQ"
   },
   "outputs": [],
   "source": [
    "# Load train/val/test subframe IDs\n",
    "def load_text_ids(file_path):\n",
    "    \"\"\"Simple helper to load all lines from a text file\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    return lines\n",
    "\n",
    "# Load the subframe names for the three data subsets\n",
    "train_ids = load_text_ids('./train_source_images.txt')\n",
    "validate_ids = load_text_ids('./val_source_images.txt')\n",
    "test_ids = load_text_ids('./test_source_images.txt')\n",
    "\n",
    "# Generate a list containing the dataset split for the matching subdirectory names\n",
    "subdir_splits = []\n",
    "for src_id in src_image_ids:\n",
    "    if src_id in train_ids:\n",
    "        subdir_splits.append('train')\n",
    "    elif src_id in validate_ids:\n",
    "        subdir_splits.append('validate')\n",
    "    elif(src_id in test_ids):\n",
    "        subdir_splits.append('test')\n",
    "    else:\n",
    "        logging.warning(f'{src_id}: Did not find designated split in train/validate/test list.')\n",
    "        subdir_splits.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCNjY_QH-sMR"
   },
   "source": [
    "# (b) Loading and pre processing the data\n",
    "### Note that there are multiple ways to preprocess and load your data in order to train your model in tensorflow. We have provided one way to do it in the following cell. Feel free to use your own method and get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eGKteSJM-sMS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:39:28 | INFO  | utils           | NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "def load_and_preprocess(img_loc, label):\n",
    "\n",
    "    def _inner_function(img_loc, label):\n",
    "        img_loc_str = img_loc.numpy().decode('utf-8')\n",
    "        img = Image.open(img_loc_str).convert('RGB')\n",
    "        img = np.array(img)\n",
    "        img = tf.image.resize(img, [299, 299])\n",
    "        img = img / 255.0\n",
    "        label = 1 if label.numpy().decode('utf-8') == 'frost' else 0\n",
    "        return img, label\n",
    "\n",
    "    X, y = tf.py_function(_inner_function, [img_loc, label], [tf.float32, tf.int64])\n",
    "    X.set_shape([299, 299, 3])\n",
    "    y.set_shape([])\n",
    "    return X, y\n",
    "\n",
    "def load_subdir_data(dir_path, image_size, seed=None):\n",
    "\n",
    "    \"\"\"Helper to create a TF dataset from each image subdirectory\"\"\"\n",
    "\n",
    "    # Grab only the classes that (1) we want to keep and (2) exist in this directory\n",
    "    tile_dir = dir_path / Path('tiles')\n",
    "    label_dir = dir_path /Path('labels')\n",
    "\n",
    "    loc_list = []\n",
    "\n",
    "    for folder in os.listdir(tile_dir):\n",
    "        if os.path.isdir(os.path.join(tile_dir, folder)):\n",
    "            for file in os.listdir(os.path.join(tile_dir, folder)):\n",
    "                if file.endswith(\".png\"):\n",
    "                    loc_list.append((os.path.join(os.path.join(tile_dir, folder), file), folder))\n",
    "\n",
    "    return loc_list\n",
    "\n",
    "# Loop over all subframes, loading each into a list\n",
    "tf_data_train, tf_data_test, tf_data_val = [], [], []\n",
    "tf_dataset_train, tf_dataset_test, tf_dataset_val = [], [], []\n",
    "\n",
    "# Update the batch and buffer size as per your model requirements\n",
    "buffer_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "for subdir, split in zip(subdirs, subdir_splits):\n",
    "    full_path = data_head_dir / subdir\n",
    "    if split=='validate':\n",
    "        tf_data_val.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "    elif split=='train':\n",
    "        tf_data_train.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "    elif split=='test':\n",
    "        tf_data_test.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "\n",
    "random.shuffle(tf_data_train)\n",
    "img_list, label_list = zip(*tf_data_train)\n",
    "img_list_t = tf.convert_to_tensor(img_list)\n",
    "lb_list_t = tf.convert_to_tensor(label_list)\n",
    "\n",
    "tf_dataset_train = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
    "tf_dataset_train = tf_dataset_train.map(load_and_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_train = tf_dataset_train.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "\n",
    "random.shuffle(tf_data_val)\n",
    "img_list, label_list = zip(*tf_data_val)\n",
    "img_list_t = tf.convert_to_tensor(img_list)\n",
    "lb_list_t = tf.convert_to_tensor(label_list)\n",
    "\n",
    "tf_dataset_val = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
    "tf_dataset_val = tf_dataset_val.map(load_and_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_val = tf_dataset_val.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "\n",
    "random.shuffle(tf_data_test)\n",
    "img_list, label_list = zip(*tf_data_train)\n",
    "img_list_t = tf.convert_to_tensor(img_list)\n",
    "lb_list_t = tf.convert_to_tensor(label_list)\n",
    "\n",
    "tf_dataset_test = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
    "tf_dataset_test = tf_dataset_test.map(load_and_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_test = tf_dataset_test.shuffle(buffer_size=buffer_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2XWyGikBPrB",
    "outputId": "85036a09-ebea-4fc2-fcb3-102469d8c53f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29679"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e92nGutNB6nG",
    "outputId": "a13f5e3e-e6a6-4c1f-978d-90efdd44436d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12823"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umb4cztiCDbh",
    "outputId": "6ab54820-3b49-43ea-e447-bf787713dc89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11286"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YHuJ7j37KFEg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "t9OOe2OYc-zr"
   },
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(\n",
    "    rotation_range=20,        # Random rotations in a range of 20 degrees\n",
    "    width_shift_range=0.2,    # Random horizontal shifts\n",
    "    height_shift_range=0.2,   # Random vertical shifts\n",
    "    shear_range=0.2,          # Shear transformations\n",
    "    zoom_range=0.2,           # Random zoom\n",
    "    horizontal_flip=True,     # Random horizontal flips\n",
    "    vertical_flip=True,       # Random vertical flips\n",
    "    fill_mode='nearest',      # Strategy to fill newly created pixels\n",
    "    brightness_range=[0.8,1.2] # Random brightness adjustments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPlJ014KetLB",
    "outputId": "b3e2d10c-005d-4e84-94fb-1552c8544ab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "                                          image_path       label\n",
      "0  data/ESP_019251_2385_15360_20480_10240_15360/t...  background\n",
      "1  data/ESP_032345_2385_10240_15360_5120_10240/ti...       frost\n",
      "2  data/ESP_066104_2230_10240_15360_15360_20480/t...       frost\n",
      "3  data/ESP_019251_2385_10240_15360_15360_20480/t...  background\n",
      "4  data/ESP_020415_1195_10240_15360_0_5120/tiles/...       frost\n",
      "\n",
      "Validation Data:\n",
      "                                          image_path       label\n",
      "0  data/ESP_048733_2075_10240_15360_10240_13427/t...  background\n",
      "1  data/ESP_064703_1170_46080_51200_5120_10240/ti...       frost\n",
      "2  data/ESP_066879_2410_15360_20480_5120_10240/ti...       frost\n",
      "3  data/PSP_005315_1770_5120_10240_0_5120/tiles/b...  background\n",
      "4  data/ESP_065389_2400_5120_10240_10240_15360/ti...       frost\n",
      "\n",
      "Test Data:\n",
      "                                          image_path       label\n",
      "0  data/ESP_070763_1790_5120_10240_0_5120/tiles/b...  background\n",
      "1  data/ESP_067517_1430_30720_35840_5120_10240/ti...  background\n",
      "2  data/ESP_019207_2450_5120_10240_0_5120/tiles/b...  background\n",
      "3  data/ESP_017717_2450_25600_30720_5120_10240/ti...  background\n",
      "4  data/PSP_006825_2465_56320_61440_76800_81920/t...       frost\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming tf_data_train is a list of tuples in the format (image_path, label)\n",
    "df_train = pd.DataFrame(tf_data_train, columns=['image_path', 'label'])\n",
    "df_val = pd.DataFrame(tf_data_val, columns=['image_path', 'label'])\n",
    "df_test = pd.DataFrame(tf_data_test, columns=['image_path', 'label'])\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\nValidation Data:\")\n",
    "print(df_val.head())\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-FDh1Rzm6ua",
    "outputId": "0ac3b8b7-9d85-43d4-883f-9fa07c002a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29679 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = image_gen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    x_col='image_path',  # Column in dataframe that contains the paths to the images\n",
    "    y_col='label',       # Column in dataframe that contains the labels\n",
    "    target_size=(299, 299),  # Target size for the images, to match model input\n",
    "    class_mode='binary',    # Class mode for labels (binary, categorical, etc.)\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "W6xRiYNxp60I"
   },
   "outputs": [],
   "source": [
    "val_gen = ImageDataGenerator()\n",
    "test_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isGuBb98qxwi",
    "outputId": "cb5adc48-8486-4899-ff62-2dc92f4071ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11286 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = val_gen.flow_from_dataframe(\n",
    "    dataframe=df_val,\n",
    "    x_col='image_path',  # Column in dataframe that contains the paths to the images\n",
    "    y_col='label',       # Column in dataframe that contains the labels\n",
    "    target_size=(299, 299),  # Target size for the images, to match model input\n",
    "    class_mode='binary',    # Class mode for labels (binary, categorical, etc.)\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4W6VsvufrPJT",
    "outputId": "3f9c5580-1431-4ac0-f327-1f503403165b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12823 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_gen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    x_col='image_path',  # Column in dataframe that contains the paths to the images\n",
    "    y_col='label',       # Column in dataframe that contains the labels\n",
    "    target_size=(299, 299),  # Target size for the images, to match model input\n",
    "    class_mode='binary',    # Class mode for labels (binary, categorical, etc.)\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) Training CNN + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_P1oBBIjrTt_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    # First Convolutional Layer\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(299, 299, 3), kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Third Convolutional Layer\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten and Dense Layers\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation='softmax')  # Change '10' to the number of classes in your dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6eRRyEMVrpOj"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # Use 'binary_crossentropy' for binary classification\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "le-WXX2brsUY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "\n",
    "checkpoint_path = 'best_model_weights.h5'\n",
    "\n",
    "# Create the ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                      monitor='val_loss',\n",
    "                                      save_best_only=True,\n",
    "                                      mode='max',\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "l0rDRr-Jrwf8",
    "outputId": "97bd8005-b704-4d63-ef6a-92894db24cb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 3.0447 - accuracy: 0.6489\n",
      "Epoch 1: val_loss improved from -inf to 2.13729, saving model to best_model_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927/927 [==============================] - 1032s 1s/step - loss: 3.0447 - accuracy: 0.6489 - val_loss: 2.1373 - val_accuracy: 0.4124\n",
      "Epoch 2/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 1.1162 - accuracy: 0.6989\n",
      "Epoch 2: val_loss improved from 2.13729 to 15.34250, saving model to best_model_weights.h5\n",
      "927/927 [==============================] - 1076s 1s/step - loss: 1.1162 - accuracy: 0.6989 - val_loss: 15.3425 - val_accuracy: 0.3480\n",
      "Epoch 3/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 1.6145 - accuracy: 0.6002\n",
      "Epoch 3: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 1040s 1s/step - loss: 1.6145 - accuracy: 0.6002 - val_loss: 1.2072 - val_accuracy: 0.3681\n",
      "Epoch 4/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 1.0366 - accuracy: 0.6522\n",
      "Epoch 4: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 1028s 1s/step - loss: 1.0366 - accuracy: 0.6522 - val_loss: 1.1272 - val_accuracy: 0.3730\n",
      "Epoch 5/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.9255 - accuracy: 0.6791\n",
      "Epoch 5: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 1023s 1s/step - loss: 0.9255 - accuracy: 0.6791 - val_loss: 0.7009 - val_accuracy: 0.8289\n",
      "Epoch 6/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.9709 - accuracy: 0.7077\n",
      "Epoch 6: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 1031s 1s/step - loss: 0.9709 - accuracy: 0.7077 - val_loss: 1.8807 - val_accuracy: 0.3224\n",
      "Epoch 7/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 1.0081 - accuracy: 0.7251\n",
      "Epoch 7: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 1015s 1s/step - loss: 1.0081 - accuracy: 0.7251 - val_loss: 0.7488 - val_accuracy: 0.6508\n",
      "Epoch 8/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.6978 - accuracy: 0.7866\n",
      "Epoch 8: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 940s 1s/step - loss: 0.6978 - accuracy: 0.7866 - val_loss: 3.4947 - val_accuracy: 0.3367\n",
      "Epoch 9/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.8089\n",
      "Epoch 9: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 940s 1s/step - loss: 0.6028 - accuracy: 0.8089 - val_loss: 0.8542 - val_accuracy: 0.5803\n",
      "Epoch 10/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.5665 - accuracy: 0.8357\n",
      "Epoch 10: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 950s 1s/step - loss: 0.5665 - accuracy: 0.8357 - val_loss: 2.4874 - val_accuracy: 0.4461\n",
      "Epoch 11/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.5250 - accuracy: 0.8558\n",
      "Epoch 11: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 939s 1s/step - loss: 0.5250 - accuracy: 0.8558 - val_loss: 2.1172 - val_accuracy: 0.3245\n",
      "Epoch 12/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.8748\n",
      "Epoch 12: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 938s 1s/step - loss: 0.4786 - accuracy: 0.8748 - val_loss: 1.0954 - val_accuracy: 0.8065\n",
      "Epoch 13/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.4492 - accuracy: 0.8874\n",
      "Epoch 13: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 937s 1s/step - loss: 0.4492 - accuracy: 0.8874 - val_loss: 4.9522 - val_accuracy: 0.3254\n",
      "Epoch 14/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.4199 - accuracy: 0.8989\n",
      "Epoch 14: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 938s 1s/step - loss: 0.4199 - accuracy: 0.8989 - val_loss: 3.7355 - val_accuracy: 0.3824\n",
      "Epoch 15/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.3933 - accuracy: 0.9093\n",
      "Epoch 15: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 936s 1s/step - loss: 0.3933 - accuracy: 0.9093 - val_loss: 2.2402 - val_accuracy: 0.8137\n",
      "Epoch 16/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.9158\n",
      "Epoch 16: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 934s 1s/step - loss: 0.3883 - accuracy: 0.9158 - val_loss: 0.7161 - val_accuracy: 0.7684\n",
      "Epoch 17/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.9221\n",
      "Epoch 17: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 936s 1s/step - loss: 0.3642 - accuracy: 0.9221 - val_loss: 0.7199 - val_accuracy: 0.7528\n",
      "Epoch 18/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.9269\n",
      "Epoch 18: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 934s 1s/step - loss: 0.3526 - accuracy: 0.9269 - val_loss: 1.2755 - val_accuracy: 0.6028\n",
      "Epoch 19/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.3470 - accuracy: 0.9297\n",
      "Epoch 19: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 933s 1s/step - loss: 0.3470 - accuracy: 0.9297 - val_loss: 5.8071 - val_accuracy: 0.3295\n",
      "Epoch 20/20\n",
      "927/927 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.9344\n",
      "Epoch 20: val_loss did not improve from 15.34250\n",
      "927/927 [==============================] - 938s 1s/step - loss: 0.3288 - accuracy: 0.9344 - val_loss: 0.7925 - val_accuracy: 0.8267\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "    validation_steps=val_generator.n // val_generator.batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x2dd1e6c80>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = keras.models.load_model('best_model_weights.h5')\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "XHHRItyclvkL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1603/1603 [==============================] - 100s 62ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.38      0.00      0.01      4418\n",
      "       frost       0.66      1.00      0.79      8405\n",
      "\n",
      "    accuracy                           0.65     12823\n",
      "   macro avg       0.52      0.50      0.40     12823\n",
      "weighted avg       0.56      0.65      0.52     12823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming test_generator is your test data generator\n",
    "test_generator.reset()  # Resetting the generator to ensure it's at the start\n",
    "predictions = best_model.predict(test_generator, steps=np.ceil(test_generator.n / test_generator.batch_size))\n",
    "\n",
    "# For binary classification, convert probabilities to binary predictions\n",
    "# For multi-class classification, use np.argmax(predictions, axis=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true labels from the generator\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Calculate metrics\n",
    "report = classification_report(true_classes, predicted_classes, target_names=test_generator.class_indices.keys())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d) Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50, VGG16\n",
    "\n",
    "base_model_efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "base_model_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "base_model_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the entire models\n",
    "base_model_efficientnet.trainable = False\n",
    "base_model_resnet.trainable = False\n",
    "base_model_vgg16.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def add_top_layer(base_model, num_classes):\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)  # Add a global spatial average pooling layer\n",
    "    x = Dense(512, activation='relu')(x)  # Add a fully-connected layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)  # Add a logistic layer for classes\n",
    "    return Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Adjust 'num_classes' as per your dataset\n",
    "model_efficientnet = add_top_layer(base_model_efficientnet, 2)\n",
    "model_resnet = add_top_layer(base_model_resnet, 2)\n",
    "model_vgg16 = add_top_layer(base_model_vgg16, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_efficientnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_vgg16.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training EfficientNet Last MLP Layer + Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29679 validated image filenames belonging to 2 classes.\n",
      "Found 11286 validated image filenames belonging to 2 classes.\n",
      "Found 12823 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9289\n",
      "Epoch 1: val_loss improved from -inf to 0.59711, saving model to eff_net_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3709/3709 [==============================] - 1743s 470ms/step - loss: 0.1777 - accuracy: 0.9289 - val_loss: 0.5971 - val_accuracy: 0.8183\n",
      "Epoch 2/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9603\n",
      "Epoch 2: val_loss did not improve from 0.59711\n",
      "3709/3709 [==============================] - 1733s 467ms/step - loss: 0.1034 - accuracy: 0.9603 - val_loss: 0.4380 - val_accuracy: 0.8622\n",
      "Epoch 3/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9659\n",
      "Epoch 3: val_loss improved from 0.59711 to 0.87121, saving model to eff_net_weights.h5\n",
      "3709/3709 [==============================] - 1774s 478ms/step - loss: 0.0852 - accuracy: 0.9659 - val_loss: 0.8712 - val_accuracy: 0.8334\n",
      "Epoch 4/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9736\n",
      "Epoch 4: val_loss did not improve from 0.87121\n",
      "3709/3709 [==============================] - 1786s 482ms/step - loss: 0.0703 - accuracy: 0.9736 - val_loss: 0.5884 - val_accuracy: 0.8544\n",
      "Epoch 5/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9761\n",
      "Epoch 5: val_loss did not improve from 0.87121\n",
      "3709/3709 [==============================] - 1784s 481ms/step - loss: 0.0629 - accuracy: 0.9761 - val_loss: 0.8052 - val_accuracy: 0.8401\n",
      "Epoch 6/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9780\n",
      "Epoch 6: val_loss improved from 0.87121 to 1.48013, saving model to eff_net_weights.h5\n",
      "3709/3709 [==============================] - 2269s 612ms/step - loss: 0.0621 - accuracy: 0.9780 - val_loss: 1.4801 - val_accuracy: 0.7456\n",
      "Epoch 7/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9790\n",
      "Epoch 7: val_loss did not improve from 1.48013\n",
      "3709/3709 [==============================] - 3033s 818ms/step - loss: 0.0559 - accuracy: 0.9790 - val_loss: 0.8472 - val_accuracy: 0.8307\n",
      "Epoch 8/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9800\n",
      "Epoch 8: val_loss improved from 1.48013 to 1.91274, saving model to eff_net_weights.h5\n",
      "3709/3709 [==============================] - 2209s 596ms/step - loss: 0.0548 - accuracy: 0.9800 - val_loss: 1.9127 - val_accuracy: 0.7387\n",
      "Epoch 9/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9816\n",
      "Epoch 9: val_loss did not improve from 1.91274\n",
      "3709/3709 [==============================] - 2419s 652ms/step - loss: 0.0516 - accuracy: 0.9816 - val_loss: 1.4105 - val_accuracy: 0.7723\n",
      "Epoch 10/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9825\n",
      "Epoch 10: val_loss did not improve from 1.91274\n",
      "3709/3709 [==============================] - 2363s 637ms/step - loss: 0.0471 - accuracy: 0.9825 - val_loss: 0.9749 - val_accuracy: 0.8194\n",
      "Epoch 11/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9829\n",
      "Epoch 11: val_loss did not improve from 1.91274\n",
      "3709/3709 [==============================] - 2313s 624ms/step - loss: 0.0474 - accuracy: 0.9829 - val_loss: 1.0157 - val_accuracy: 0.8373\n",
      "Epoch 12/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9847\n",
      "Epoch 12: val_loss did not improve from 1.91274\n",
      "3709/3709 [==============================] - 1808s 487ms/step - loss: 0.0435 - accuracy: 0.9847 - val_loss: 1.2096 - val_accuracy: 0.8181\n",
      "Epoch 13/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9839\n",
      "Epoch 13: val_loss did not improve from 1.91274\n",
      "3709/3709 [==============================] - 824s 222ms/step - loss: 0.0441 - accuracy: 0.9839 - val_loss: 1.4542 - val_accuracy: 0.7998\n",
      "Epoch 14/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9848\n",
      "Epoch 14: val_loss did not improve from 1.91274\n",
      "3709/3709 [==============================] - 818s 220ms/step - loss: 0.0438 - accuracy: 0.9848 - val_loss: 1.1854 - val_accuracy: 0.8144\n",
      "Epoch 15/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9848\n",
      "Epoch 15: val_loss did not improve from 1.91274\n",
      "3709/3709 [==============================] - 1260s 340ms/step - loss: 0.0396 - accuracy: 0.9848 - val_loss: 1.4316 - val_accuracy: 0.8244\n",
      "Epoch 16/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9854\n",
      "Epoch 16: val_loss did not improve from 1.91274\n",
      "3709/3709 [==============================] - 806s 217ms/step - loss: 0.0391 - accuracy: 0.9854 - val_loss: 1.3058 - val_accuracy: 0.8190\n",
      "Epoch 17/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9862\n",
      "Epoch 17: val_loss did not improve from 1.91274\n",
      "3709/3709 [==============================] - 821s 221ms/step - loss: 0.0382 - accuracy: 0.9862 - val_loss: 1.2800 - val_accuracy: 0.8348\n",
      "Epoch 18/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9861\n",
      "Epoch 18: val_loss improved from 1.91274 to 2.09029, saving model to eff_net_weights.h5\n",
      "3709/3709 [==============================] - 824s 222ms/step - loss: 0.0386 - accuracy: 0.9861 - val_loss: 2.0903 - val_accuracy: 0.7688\n",
      "Epoch 19/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9870\n",
      "Epoch 19: val_loss did not improve from 2.09029\n",
      "3709/3709 [==============================] - 836s 225ms/step - loss: 0.0366 - accuracy: 0.9870 - val_loss: 1.9196 - val_accuracy: 0.7823\n",
      "Epoch 20/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9863\n",
      "Epoch 20: val_loss did not improve from 2.09029\n",
      "3709/3709 [==============================] - 828s 223ms/step - loss: 0.0389 - accuracy: 0.9863 - val_loss: 0.9340 - val_accuracy: 0.8455\n"
     ]
    }
   ],
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "checkpoint_path = 'eff_net_weights.h5'\n",
    "\n",
    "# Create the ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                      monitor='val_loss',\n",
    "                                      save_best_only=True,\n",
    "                                      mode='max',\n",
    "                                      verbose=1)\n",
    "\n",
    "train_generator = image_gen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    x_col='image_path',  # Column in dataframe that contains the paths to the images\n",
    "    y_col='label',       # Column in dataframe that contains the labels\n",
    "    target_size=(299, 299),  # Target size for the images, to match model input\n",
    "    class_mode='binary',    # Class mode for labels (binary, categorical, etc.)\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = ImageDataGenerator()\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "val_generator = val_gen.flow_from_dataframe(\n",
    "    dataframe=df_val,\n",
    "    x_col='image_path',  # Column in dataframe that contains the paths to the images\n",
    "    y_col='label',       # Column in dataframe that contains the labels\n",
    "    target_size=(299, 299),  # Target size for the images, to match model input\n",
    "    class_mode='binary',    # Class mode for labels (binary, categorical, etc.)\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = test_gen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    x_col='image_path',  # Column in dataframe that contains the paths to the images\n",
    "    y_col='label',       # Column in dataframe that contains the labels\n",
    "    target_size=(299, 299),  # Target size for the images, to match model input\n",
    "    class_mode='binary',    # Class mode for labels (binary, categorical, etc.)\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history = model_efficientnet.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "    validation_steps=val_generator.n // val_generator.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.functional.Functional at 0x2dd1a6350>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = keras.models.load_model('eff_net_weights.h5')\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1603/1603 [==============================] - 227s 141ms/step\n",
      "Efficient Net Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.35      0.31      0.33      4418\n",
      "       frost       0.66      0.70      0.68      8405\n",
      "\n",
      "    accuracy                           0.57     12823\n",
      "   macro avg       0.51      0.50      0.50     12823\n",
      "weighted avg       0.55      0.57      0.56     12823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming test_generator is your test data generator\n",
    "test_generator.reset()  # Resetting the generator to ensure it's at the start\n",
    "predictions = best_model.predict(test_generator, steps=np.ceil(test_generator.n / test_generator.batch_size))\n",
    "\n",
    "# For binary classification, convert probabilities to binary predictions\n",
    "# For multi-class classification, use np.argmax(predictions, axis=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true labels from the generator\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Calculate metrics\n",
    "report = classification_report(true_classes, predicted_classes, target_names=test_generator.class_indices.keys())\n",
    "print(\"Efficient Net Classification Report: \")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RestNet Last MLP Layer + Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interrupting the epochs of ResNet below due to lack of resources (time & processing capabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9294\n",
      "Epoch 1: val_loss improved from -inf to 0.42256, saving model to res_net_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3709/3709 [==============================] - 4795s 1s/step - loss: 0.1760 - accuracy: 0.9294 - val_loss: 0.4226 - val_accuracy: 0.8750\n",
      "Epoch 2/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0996 - accuracy: 0.9617\n",
      "Epoch 2: val_loss did not improve from 0.42256\n",
      "3709/3709 [==============================] - 4892s 1s/step - loss: 0.0996 - accuracy: 0.9617 - val_loss: 0.4003 - val_accuracy: 0.8724\n",
      "Epoch 3/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9700\n",
      "Epoch 3: val_loss improved from 0.42256 to 1.03850, saving model to res_net_weights.h5\n",
      "3709/3709 [==============================] - 4880s 1s/step - loss: 0.0776 - accuracy: 0.9700 - val_loss: 1.0385 - val_accuracy: 0.7871\n",
      "Epoch 4/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9744\n",
      "Epoch 4: val_loss did not improve from 1.03850\n",
      "3709/3709 [==============================] - 4852s 1s/step - loss: 0.0666 - accuracy: 0.9744 - val_loss: 0.6980 - val_accuracy: 0.8316\n",
      "Epoch 5/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9773\n",
      "Epoch 5: val_loss improved from 1.03850 to 1.62955, saving model to res_net_weights.h5\n",
      "3709/3709 [==============================] - 4867s 1s/step - loss: 0.0603 - accuracy: 0.9773 - val_loss: 1.6296 - val_accuracy: 0.6921\n",
      "Epoch 6/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9791\n",
      "Epoch 6: val_loss did not improve from 1.62955\n",
      "3709/3709 [==============================] - 4878s 1s/step - loss: 0.0551 - accuracy: 0.9791 - val_loss: 1.1132 - val_accuracy: 0.7556\n",
      "Epoch 7/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9813\n",
      "Epoch 7: val_loss did not improve from 1.62955\n",
      "3709/3709 [==============================] - 5220s 1s/step - loss: 0.0513 - accuracy: 0.9813 - val_loss: 0.9888 - val_accuracy: 0.7832\n",
      "Epoch 8/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9834\n",
      "Epoch 8: val_loss improved from 1.62955 to 1.87671, saving model to res_net_weights.h5\n",
      "3709/3709 [==============================] - 5738s 2s/step - loss: 0.0447 - accuracy: 0.9834 - val_loss: 1.8767 - val_accuracy: 0.6978\n",
      "Epoch 9/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9833\n",
      "Epoch 9: val_loss improved from 1.87671 to 2.10545, saving model to res_net_weights.h5\n",
      "3709/3709 [==============================] - 4878s 1s/step - loss: 0.0444 - accuracy: 0.9833 - val_loss: 2.1055 - val_accuracy: 0.6170\n",
      "Epoch 10/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9843\n",
      "Epoch 10: val_loss did not improve from 2.10545\n",
      "3709/3709 [==============================] - 4853s 1s/step - loss: 0.0421 - accuracy: 0.9843 - val_loss: 1.2577 - val_accuracy: 0.7456\n",
      "Epoch 11/20\n",
      "3709/3709 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9852\n",
      "Epoch 11: val_loss did not improve from 2.10545\n",
      "3709/3709 [==============================] - 4893s 1s/step - loss: 0.0415 - accuracy: 0.9852 - val_loss: 2.0240 - val_accuracy: 0.7044\n",
      "Epoch 12/20\n",
      " 792/3709 [=====>........................] - ETA: 46:29 - loss: 0.0414 - accuracy: 0.9856"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create the ModelCheckpoint callback to save the best model based on validation accuracy\u001b[39;00m\n\u001b[1;32m      6\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mcheckpoint_path,\n\u001b[1;32m      7\u001b[0m                                       monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                       save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m                                       mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                       verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_resnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "checkpoint_path = 'res_net_weights.h5'\n",
    "\n",
    "# Create the ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                      monitor='val_loss',\n",
    "                                      save_best_only=True,\n",
    "                                      mode='max',\n",
    "                                      verbose=1)\n",
    "\n",
    "history = model_resnet.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "    validation_steps=val_generator.n // val_generator.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.functional.Functional at 0x2de0af280>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = keras.models.load_model('res_net_weights.h5')\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1603/1603 [==============================] - 1546s 964ms/step\n",
      "Res Net Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.34      0.26      0.30      4418\n",
      "       frost       0.65      0.73      0.69      8405\n",
      "\n",
      "    accuracy                           0.57     12823\n",
      "   macro avg       0.50      0.50      0.49     12823\n",
      "weighted avg       0.55      0.57      0.56     12823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming test_generator is your test data generator\n",
    "test_generator.reset()  # Resetting the generator to ensure it's at the start\n",
    "predictions = best_model.predict(test_generator, steps=np.ceil(test_generator.n / test_generator.batch_size))\n",
    "\n",
    "# For binary classification, convert probabilities to binary predictions\n",
    "# For multi-class classification, use np.argmax(predictions, axis=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true labels from the generator\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Calculate metrics\n",
    "report = classification_report(true_classes, predicted_classes, target_names=test_generator.class_indices.keys())\n",
    "print(\"Res Net Classification Report: \")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training VGG16 Last MLP Layer + Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "checkpoint_path = 'vgg_weights.h5'\n",
    "\n",
    "# Create the ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                      monitor='val_loss',\n",
    "                                      save_best_only=True,\n",
    "                                      mode='max',\n",
    "                                      verbose=1)\n",
    "\n",
    "history = model_vgg16.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "    validation_steps=val_generator.n // val_generator.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model('vgg_weights.h5')\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_generator is your test data generator\n",
    "test_generator.reset()  # Resetting the generator to ensure it's at the start\n",
    "predictions = best_model.predict(test_generator, steps=np.ceil(test_generator.n / test_generator.batch_size))\n",
    "\n",
    "# For binary classification, convert probabilities to binary predictions\n",
    "# For multi-class classification, use np.argmax(predictions, axis=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true labels from the generator\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Calculate metrics\n",
    "report = classification_report(true_classes, predicted_classes, target_names=test_generator.class_indices.keys())\n",
    "print(\"VGG 16 Classification Report: \")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could not train VGG16 last layer due to lack of resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the results of CNN+MLP , EfficeintNet, RestNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+MLP Model (First Image):\n",
    "\n",
    "Precision: The model has low precision for the 'background' class (0.38) and much better precision for the 'frost' class (0.66).\n",
    "\n",
    "Recall: The recall is very low for 'background' (0.00), meaning the model failed to correctly identify any of the 'background' instances.\n",
    "However, it has perfect recall for 'frost' (1.00), identifying all 'frost' instances correctly.\n",
    "\n",
    "F1-Score: Due to the poor recall for 'background', the F1-score is very low (0.01), while it is high for 'frost' (0.79).\n",
    "\n",
    "Support: The number of actual occurrences of each class in the dataset shows that 'frost' instances are nearly double the 'background' instances.\n",
    "\n",
    "Overall Accuracy: 0.65\n",
    "\n",
    "\n",
    "#### EfficientNet Model :\n",
    "\n",
    "Precision: Similar to the CNN+MLP model, this model also has better precision for 'frost' (0.66) than for 'background' (0.35).\n",
    "\n",
    "Recall: The recall for 'background' has improved (0.31), and is lower for 'frost' (0.70) compared to the CNN+MLP model.\n",
    "\n",
    "F1-Score: The F1-scores for both classes are more balanced (0.33 for 'background' and 0.68 for 'frost'), reflecting a better balance between precision and recall.\n",
    "\n",
    "Overall Accuracy: 0.57\n",
    "\n",
    "\n",
    "#### ResNet Model :\n",
    "\n",
    "Precision: Similar to the other two models, the precision for 'frost' (0.65) is higher than for 'background' (0.34).\n",
    "\n",
    "Recall: The recall for 'background' is worse than the EfficientNet model (0.26), but better than the CNN+MLP model. For 'frost', the recall (0.73) is higher than EfficientNet but lower than CNN+MLP.\n",
    "\n",
    "F1-Score: The F1-scores are closer to each other (0.30 for 'background' and 0.69 for 'frost'), indicating a balance between precision and recall.\n",
    "\n",
    "Overall Accuracy: 0.57\n",
    "\n",
    "\n",
    "### Comparison and Analysis:\n",
    "\n",
    "The CNN+MLP model has a high recall for the 'frost' class but fails entirely for 'background', which suggests overfitting to the 'frost' class.\n",
    "\n",
    "EfficientNet, a transfer learning model, shows a more balanced performance between the two classes and improves the recall for 'background' significantly, which suggests that it generalizes better than the CNN+MLP model.\n",
    "\n",
    "ResNet, another transfer learning model, shows slightly less recall for 'frost' than EfficientNet but improves slightly over EfficientNet's recall for 'background'. It seems to be a middle ground between the CNN+MLP model and EfficientNet in terms of balancing the recall between both classes.\n",
    "\n",
    "### Transfer Learning Advantage:\n",
    "\n",
    "Transfer learning models like EfficientNet and ResNet generally perform better because they use pre-trained weights from models trained on large datasets like ImageNet. These weights provide a better starting point for feature extraction, even for datasets that are significantly different from the training data used in the pre-training.\n",
    "\n",
    "In this case, both transfer learning models show a more balanced performance across classes than the CNN+MLP model, which may be overfitting to the 'frost' class due to imbalanced training data or not having a robust enough feature extraction capability to differentiate 'background' effectively.\n",
    "\n",
    "Moreover, while the accuracy of the CNN+MLP is higher than the transfer learning models, this metric is misleading in this case because of the severe class imbalance in recall for the CNN+MLP model. The balanced F1-scores for EfficientNet and ResNet provide a more realistic picture of their performance, especially in the context of an imbalanced dataset.\n",
    "\n",
    "In conclusion, transfer learning models are demonstrating their ability to leverage pre-trained knowledge, leading to a better generalization and a more balanced performance across the classes, despite having a slightly lower overall accuracy in this particular scenario."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
